{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ab874",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import librosa\n",
    "from soundfile import write\n",
    "import music21\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import math\n",
    "from itertools import permutations\n",
    "import csv\n",
    "from fractions import Fraction\n",
    "\n",
    "from performer.models.ddsp_module import DDSP\n",
    "from performer.datamodules.components.ddsp_dataset import DDSPDataset\n",
    "\n",
    "from IPython.display import Audio, Image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e50f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795722d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flt_ckpt = '../checkpoints/flute_longrun.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    model = DDSP.load_from_checkpoint(flt_ckpt, map_location='cuda')\n",
    "    model = model.to('cuda')\n",
    "    model.eval()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e6da",
   "metadata": {},
   "source": [
    "## Another mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b65d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport performer.composition.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46045dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expo = performer.composition.score.Envelope(0.1, 3.5, 0., 0.8)\n",
    "t = np.linspace(-1, 4, 1250)\n",
    "plt.plot(t, expo(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f889199",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = performer.composition.score.NoteList()\n",
    "start, duration, gap = 1.0, 0.2, 0.2\n",
    "for i in range(8):\n",
    "    notes.append(performer.composition.score.Note(start + gap * i, duration, 0.6, 110. * (i+3)))\n",
    "t, env, f0 = notes.render()\n",
    "\n",
    "adsr = env * 90 - 100\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y = model(torch.from_numpy(f0[None, None, :].astype('float32')).cuda(), torch.from_numpy(adsr[None, None, :].astype('float32')).cuda())\n",
    "\n",
    "plt.plot(t, env)\n",
    "plt.show()\n",
    "\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54434502",
   "metadata": {},
   "source": [
    "## Examine real loudness and f0 envelopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = \"/home/kureta/Music/Flute Samples/02. Fantasia No. 1 in A Major, TWV 40_2.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performer.utils.features import Loudness, get_f0\n",
    "from performer.utils.constants import N_FFT\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e539c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = Loudness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _ = librosa.load(sample_path, sr=48000, mono=False, dtype='float32', duration=15.)\n",
    "audio = torch.from_numpy(audio)\n",
    "audio.unsqueeze_(0)\n",
    "audio = F.pad(audio, (N_FFT //2, N_FFT //2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb80169",
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness = ld.get_amp(audio)\n",
    "f0 = get_f0(audio, fmin=31.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = loudness.squeeze().cpu().numpy()\n",
    "freq = f0.squeeze().cpu().numpy()\n",
    "t = np.arange(len(amp)) / 250\n",
    "\n",
    "source = {\n",
    "    'time': t, \n",
    "    'f0': freq,\n",
    "    'amp': amp,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9073d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=t, y=freq, name=\"F0 (Hz)\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=t, y=amp, name=\"Loudness (dB)\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Flute control parameters\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"time (seconds)\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"F0 (Hz)\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Loudness (dB)\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e37bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio.squeeze().cpu()[0], rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d275cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y = model(f0, loudness)\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39715fe6",
   "metadata": {},
   "source": [
    "# Lilypond event stream parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883a2b9",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "### Loudness envelope\n",
    "\n",
    "- Set default values for `attack_time`, `decay_time`, `release_time`.\n",
    "  - If duration is less than the sum of those:\n",
    "    - If duration is also less than `attack_time` + `decay_time` set both to half the duration\n",
    "  - Else, keep `attack_time`, extend `decay_time` to the end if necessary\n",
    "  - Extend duration with a sustain as necessary, if all of the above fits into the duration\n",
    "- Set `peak_amp` to 1.0 and set a default `sustain_amp`\n",
    "  - Multiply the resulting envelope with constant values if there are any dynamic indicators (ex. ___pp___, ___f___, ...)\n",
    "  - if there are no dynamic indicators at the start of the piece, assume ___mf___.\n",
    "  - If there are hairpins, multiply with a line from starting dynamic to ending dynamic\n",
    "  - ___sfz___ modifies `peak_amp` / `sustain_amp` ratio.\n",
    "  - Decide what to do with other dynamic indicators when they come up.\n",
    "  - Finally, map the envelop values from 0-1 to `min_db`-`max_db`.\n",
    "- If there is a slur or tie, total duration of the envelope will be equal to the duration of the slur.\n",
    "  - Maybe add tiny attack/decays if note changes inside the slur.\n",
    "\n",
    "__Note__: not enough information in events to support phrasing slurs (slurs within slurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_f1 = './CanisMajoris2022-Flute I.notes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57577ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_note_sec(tempo):\n",
    "    return 60 * 16 / tempo\n",
    "\n",
    "def moment_to_sec(moment, tempo):\n",
    "    return whole_note_sec(tempo) * moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d592874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_hz(midi: float) -> float:\n",
    "    return 440. * 2**((midi - 69) / 12)\n",
    "\n",
    "def hz_to_midi(hz: float) -> float:\n",
    "    return 12 * torch.log2(hz / 440) + 69\n",
    "\n",
    "def ratio_to_interval(ratio):\n",
    "    return 12 * torch.log2(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_from_unit(value, low, high):\n",
    "    scale = high - low\n",
    "    return value * scale + low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adsr(ta, td, tr, zero, peak, sustain, dur):\n",
    "    ts = dur - ta - td - tr\n",
    "    \n",
    "    env_a = torch.linspace(zero, peak, round(ta * 250))\n",
    "    env_d = torch.linspace(peak, sustain, round(td * 250))\n",
    "    env_sus = torch.ones(round(ts * 250)) * sustain\n",
    "    env_rel = torch.linspace(sustain, zero, round(tr * 250))\n",
    "\n",
    "    env = torch.cat([env_a, env_d, env_sus, env_rel]).cuda()\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ea95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(val: float | str):\n",
    "    if isinstance(val, str):\n",
    "        return float(val)\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, row):\n",
    "        self.moment = to_float(row[0])\n",
    "        self.tempo = None\n",
    "    \n",
    "    @property\n",
    "    def time(self) -> float:\n",
    "        return moment_to_sec(self.moment, self.tempo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<{self.__class__.__name__.upper()}>\\ttime: {self.time:.2f} tempo: {self.tempo:.2f}'\n",
    "\n",
    "class Tempo(Event):\n",
    "    def __init__(self, row):\n",
    "        super().__init__(row)\n",
    "        self.tempo = to_float(row[2])\n",
    "\n",
    "class NoteOrRest(Event):\n",
    "    def __init__(self, row, tempo):\n",
    "        super().__init__(row)        \n",
    "        self.tempo = tempo\n",
    "    \n",
    "    @property\n",
    "    def dur(self):\n",
    "        return moment_to_sec(self.dur_moment, self.tempo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        parent_repr = super().__repr__()\n",
    "        return f'{parent_repr} duration: {self.dur:.2f}'\n",
    "\n",
    "class Note(NoteOrRest):\n",
    "    def __init__(self, row, tempo):\n",
    "        super().__init__(row, tempo)\n",
    "        self.pitch = to_float(row[2])\n",
    "        self.dur_moment = to_float(row[4])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        parent_repr = super().__repr__()\n",
    "        return f'{parent_repr} pitch: {self.pitch:.2f}'\n",
    "\n",
    "    @property\n",
    "    def dur(self):\n",
    "        return moment_to_sec(self.dur_moment, self.tempo)\n",
    "\n",
    "class Rest(NoteOrRest):\n",
    "    def __init__(self, row, tempo):\n",
    "        super().__init__(row, tempo)\n",
    "        self.dur_moment = to_float(row[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb07d3",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "- We need 3 objects: pitch, gate, amp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(path: str):\n",
    "    with open(path) as csvfile:        \n",
    "        current_tempo = None\n",
    "        \n",
    "        for row in csv.reader(csvfile, delimiter='\\t'):\n",
    "            match row[1]:\n",
    "                case 'tempo':\n",
    "                    tempo = Tempo(row)\n",
    "                    current_tempo = tempo.tempo\n",
    "                    yield tempo\n",
    "                case 'note':\n",
    "                    yield Note(row, current_tempo)\n",
    "                case 'rest':\n",
    "                    yield Rest(row, current_tempo)\n",
    "                case default:\n",
    "                    yield f'<NA>\\ttime: {moment_to_sec(to_float(row[0]), current_tempo):.2f} kind: {row[1]} values: {\" - \".join(row[2:])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c10a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "pitch_vals = []\n",
    "amp_vals = []\n",
    "t_vals = []\n",
    "gates = []\n",
    "for event in parser(a_f1):\n",
    "    print(event)\n",
    "    if isinstance(event, Tempo):\n",
    "        # Nothing to do yet\n",
    "        pass\n",
    "    elif isinstance(event, Note):\n",
    "        pitch_vals.append(event.pitch)\n",
    "        amp_vals.append(0.7)\n",
    "        t_vals.append(event.time)\n",
    "        pitch_vals.append(event.pitch)\n",
    "        amp_vals.append(0.7)\n",
    "        t_vals.append(event.time + event.dur - 1e-10)\n",
    "    elif isinstance(event, Rest):\n",
    "        last_pitch = pitch_vals[-1]\n",
    "        pitch_vals.append(last_pitch)\n",
    "        amp_vals.append(0.0)\n",
    "        t_vals.append(event.time)\n",
    "        pitch_vals.append(last_pitch)\n",
    "        amp_vals.append(0.0)\n",
    "        t_vals.append(event.time + event.dur - 1e-10)\n",
    "\n",
    "pitch = np.array(pitch_vals, dtype='float32')\n",
    "amp = np.array(amp_vals, dtype='float32')\n",
    "t = np.array(t_vals, dtype='float32')\n",
    "\n",
    "interp_pitch = interpolate.interp1d(t, pitch)\n",
    "interp_amp = interpolate.interp1d(t, amp)\n",
    "\n",
    "t_new = np.linspace(t[0], t[-1], round(t[-1] * 250), dtype='float32')\n",
    "\n",
    "pitch_new = interp_pitch(t_new)\n",
    "amp_new = interp_amp(t_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f47b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = torch.from_numpy(midi_to_hz(pitch_new)).cuda()\n",
    "loudness = torch.from_numpy(map_from_unit(amp_new, -100, -15)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4714555",
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness.min(), loudness.max(), amp_new.min(), amp_new.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y = model(freq[None, None, :], loudness[None, None, :]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=y.cpu(), rate=48000, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0300ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loudness.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f30f3",
   "metadata": {},
   "source": [
    "# Old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9062ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "us = music21.environment.UserSettings()\n",
    "us['musescoreDirectPNGPath'] = '/usr/bin/musescore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_hz(midi: float) -> float:\n",
    "    return 440. * 2**((midi - 69) / 12)\n",
    "\n",
    "def hz_to_midi(hz: float) -> float:\n",
    "    return 12 * torch.log2(hz / 440) + 69\n",
    "\n",
    "def ratio_to_interval(ratio):\n",
    "    return 12 * torch.log2(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e975f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adsr(ta, td, tr, zero, peak, sustain, dur):\n",
    "    ts = dur - ta - td - tr\n",
    "    \n",
    "    env_a = torch.linspace(zero, peak, round(ta * 250))\n",
    "    env_d = torch.linspace(peak, sustain, round(td * 250))\n",
    "    env_sus = torch.ones(round(ts * 250)) * sustain\n",
    "    env_rel = torch.linspace(sustain, zero, round(tr * 250))\n",
    "\n",
    "    env = torch.cat([env_a, env_d, env_sus, env_rel]).cuda()\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741beed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(ts: float, f: float):\n",
    "    t = torch.arange(int(ts * 250), dtype=torch.float32, device='cuda') / 250\n",
    "    result = torch.sin(2 * np.pi * f * t)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sin_like(ts: torch.Tensor, f: float):\n",
    "    t = torch.arange(ts.shape[-1], dtype=torch.float32, device='cuda') / 250\n",
    "    result = torch.sin(2 * np.pi * f * t)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(music):\n",
    "    display(Image(str(music.write(\"lily.png\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_microtone(note):\n",
    "    cents = note.pitch.microtone.cents\n",
    "    prefix = ''\n",
    "    if cents > 0:\n",
    "        prefix = '+'\n",
    "    if abs(cents) >= 10:\n",
    "        note.addLyric(f'{prefix}{int(np.round(cents))}', applyRaw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.Random(123)\n",
    "beat = 0.75  # 1 beat is 0.75 seconds\n",
    "fps = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_measure(p1, p2):\n",
    "    one = music21.note.Note(quarterLength=1/3)\n",
    "    one.articulations.append(music21.articulations.Accent())\n",
    "    one.pitch.frequency = p1\n",
    "    add_microtone(one)\n",
    "\n",
    "    two = music21.note.Note(quarterLength=1/3)\n",
    "    two.pitch.frequency = p2\n",
    "    two.articulations.append(music21.articulations.Staccato())\n",
    "    add_microtone(two)\n",
    "\n",
    "    sl1 = music21.spanner.Slur([one, two])\n",
    "\n",
    "    rest1 = music21.note.Rest(1/3)\n",
    "    rest = music21.note.Rest(4)\n",
    "\n",
    "    m01 = music21.stream.Measure(number=1)\n",
    "\n",
    "    # m01.append(music21.dynamics.Dynamic('sfz'))\n",
    "    m01.append(one)\n",
    "    m01.append(two)\n",
    "    m01.append(sl1)\n",
    "    m01.append(rest1)\n",
    "    m01.append(rest)\n",
    "    \n",
    "    return m01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58769d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = [2, 3, 5, 7, 11/2, 13/2, 17/4, 19/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = music21.stream.Measure()\n",
    "for val in constant:\n",
    "    nn = music21.note.Note()\n",
    "    freq = midi_to_hz(ratio_to_interval(torch.tensor(val)) + 52)\n",
    "    nn.pitch.frequency = freq\n",
    "    mm.append(nn)\n",
    "mm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    [(0, 1), (2, 7), (2, 4), (5, 2), (7, 5), (7, 6), (7, 6), (2, 5)],\n",
    "    [(7, 5), (4, 3), (7, 3), (3, 0), (1, 0), (3, 1), (4, 5), (3, 0)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d86490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = [Fraction(c) for c in constant]\n",
    "[(cons[i], cons[j]) for i, j in lines[1]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.functional.filtering import lowpass_biquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff169ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport performer.canis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "adsr = performer.canis.ADSR(0.2, 4.0)\n",
    "adsr.set_staccato()\n",
    "# adsr.set_sforzando()\n",
    "env = adsr.get_envelope_func()\n",
    "t = np.linspace(0, 8.0, 250*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, env(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    amp = env(t)\n",
    "    amp = amp * 90 - 100 - 10.\n",
    "    amp = torch.from_numpy(amp.astype('float32')).cuda()\n",
    "\n",
    "    f0 = torch.ones_like(amp, device='cuda') * 440.\n",
    "\n",
    "    y = model(f0[None, None, :], amp[None, None, :])\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46689f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "many = performer.canis.ADSRList()\n",
    "delta = 0.25\n",
    "duration = 1.5\n",
    "start = 0.5\n",
    "for idx in range(5):\n",
    "    many.notes.append(performer.canis.ADSR(start, duration))\n",
    "    start += duration\n",
    "env = many.get_envelope_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    amp = env(t)\n",
    "    dynamics = performer.canis.get_line(many.notes[0].start, duration * 5, 1, 0.5)\n",
    "    def dynamo(t):\n",
    "        return np.minimum(1, np.maximum(dynamics(t), 0))\n",
    "    amp = amp * dynamo(t)\n",
    "    amp = amp * 90 - 100 - 10.\n",
    "    amp = torch.from_numpy(amp.astype('float32')).cuda()\n",
    "\n",
    "    f0 = torch.ones_like(amp, device='cuda') * 440.\n",
    "\n",
    "    y = model(f0[None, None, :], amp[None, None, :])\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d52f2",
   "metadata": {},
   "source": [
    "## WHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = adsr(0.1, 0.7, 0.01, -100, -8, -48, 1.5 * beat)\n",
    "silence = torch.ones(round(3.5 * beat * fps), device='cuda') * -100.\n",
    "env = torch.cat([amp, silence], dim=-1)\n",
    "\n",
    "\n",
    "s = music21.stream.Score(id='mainScore')\n",
    "part0 = music21.stream.Part(id='part0')\n",
    "part1 = music21.stream.Part(id='part1')\n",
    "\n",
    "\n",
    "ys = []\n",
    "parts = [part0, part1]\n",
    "lines = [\n",
    "#     [(0, 1), (2, 4), (7, 4), (5, 2), (7, 5), (7, 6), (7, 6), (7, 6)],\n",
    "#     [(7, 5), (2, 3), (7, 3), (3, 0), (1, 0), (3, 1), (4, 5), (3, 0)],\n",
    "    [(0, 1), (2, 7), (2, 4), (5, 2), (7, 5), (7, 6), (7, 6), (2, 5)],\n",
    "    [(7, 5), (4, 3), (7, 3), (3, 0), (1, 0), (3, 1), (4, 5), (3, 0)],\n",
    "]\n",
    "for part, line in zip(parts, lines):\n",
    "    oll = []\n",
    "    for idx1, idx2 in line:\n",
    "        with torch.inference_mode():\n",
    "            p1 = midi_to_hz(ratio_to_interval(torch.tensor(constant[idx1])) + 52)\n",
    "            p2 = midi_to_hz(ratio_to_interval(torch.tensor(constant[idx2])) + 52)\n",
    "            mezura = build_measure(p1, p2)\n",
    "            # if j % 3 == 2:\n",
    "            #     mezura.append(music21.layout.SystemLayout(isNew=True))\n",
    "            part.append(mezura)\n",
    "            \n",
    "            f0 = torch.ones_like(env) * p2\n",
    "            f0[:int(beat*0.333*fps)] = p1\n",
    "            y = model(f0[None, None, :], env[None, None, :])\n",
    "            oll.append(y)\n",
    "\n",
    "    ys.append(torch.cat(oll, dim=-1).cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "tempo = music21.tempo.MetronomeMark(referent=1.0, number=90.0)\n",
    "\n",
    "for part in parts:\n",
    "    part.measure(1).insert(tempo)\n",
    "    part.insert(0, music21.meter.TimeSignature('5/4'))\n",
    "    s.insert(0, part)\n",
    "\n",
    "f0 = midi_to_hz(torch.ones_like(env, device='cuda') * 51-12)\n",
    "amp = adsr(0.1, 0.7, 0.01, -100, -3, -48, 2.5 * beat)\n",
    "silence = torch.ones(round(2.5 * beat * fps), device='cuda') * -100.\n",
    "env = torch.cat([amp, silence], dim=-1)\n",
    "oll = []\n",
    "for _ in range(8):\n",
    "    with torch.inference_mode():\n",
    "        y = model(f0[None, None, :], env[None, None, :])  # * (torch.randn(1, device='cuda') * 0.25 + 1))\n",
    "        oll.append(y)\n",
    "\n",
    "ys.append(torch.cat(oll, dim=-1).cpu().numpy().squeeze())\n",
    "\n",
    "s.show()\n",
    "Audio(data=sum(ys), rate=48000, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.min(), env.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957b229",
   "metadata": {},
   "source": [
    "## Braids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tones = np.arange(1, 4)\n",
    "print(tones)\n",
    "for i in range(4):\n",
    "    print(np.random.permutation(tones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d63c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a0476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
