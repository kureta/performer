{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ab874",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import librosa\n",
    "from soundfile import write\n",
    "import music21\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import math\n",
    "from itertools import permutations\n",
    "import csv\n",
    "from fractions import Fraction\n",
    "\n",
    "from performer.models.ddsp_module import DDSP\n",
    "from performer.datamodules.components.ddsp_dataset import DDSPDataset\n",
    "\n",
    "from IPython.display import Audio, Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "from torchaudio.functional.filtering import lowpass_biquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795722d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flt_ckpt = '../checkpoints/flute_longrun.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    model = DDSP.load_from_checkpoint(flt_ckpt, map_location='cpu')\n",
    "#     model = model.to('cuda')\n",
    "    model.eval()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6e6da",
   "metadata": {},
   "source": [
    "## Composition Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b65d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport performer.composition.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.notes\n",
    "!lilypond --include=./ CanisMajoris2022.ly\n",
    "\n",
    "path = Path('.')\n",
    "part_notes = []\n",
    "\n",
    "for file in path.glob('*.notes'):\n",
    "    part_notes.append(performer.composition.score.parser(file))\n",
    "\n",
    "ends = []\n",
    "for notes in part_notes:\n",
    "    ends.append(notes.notes[-1].t0 + notes.notes[-1].duration)\n",
    "\n",
    "start = -0.5\n",
    "end = max(ends) + 3.5\n",
    "\n",
    "# start = 30\n",
    "# end = 45\n",
    "\n",
    "t = np.linspace(start, end, int(250*(end - start)))\n",
    "\n",
    "parts = []\n",
    "for idx, notes in enumerate(part_notes):\n",
    "    env = notes.curve(t)\n",
    "    f0 = torch.from_numpy(notes.freq(t).astype('float32'))\n",
    "    \n",
    "    env = torch.from_numpy(env.astype('float32'))\n",
    "    env = lowpass_biquad(env + torch.randn_like(env) * 0.01, 250, 8.)\n",
    "    adsr = env * 90 - 100\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        p1 = f0[None, None, :]  #.cuda()\n",
    "        p2 = adsr[None, None, :]  #.cuda()\n",
    "\n",
    "#         p2 = lowpass_biquad(p2, 250, 15.)\n",
    "        y = model(p1, p2)\n",
    "    \n",
    "    parts.append(y.cpu().squeeze())\n",
    "#     del y\n",
    "#     del p1\n",
    "#     del p2\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "Audio(sum(parts), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.wav\n",
    "for idx, part in enumerate(parts):\n",
    "    sf.write(f'{idx}.wav', part.T, 48000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(parts).max(), sum(parts).min())\n",
    "Audio(sum(parts), rate=48000, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266afbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'time': t,\n",
    "    'amplitude': p2.squeeze()\n",
    "}\n",
    "\n",
    "fig = px.line(data, x='time', y='amplitude', title='Amplitude curve')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11ac6f",
   "metadata": {},
   "source": [
    "## Yet another mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [2, 3, 5, 7, 11, 13, 17, 19]\n",
    "template = \"\\\\tuplet 3/2 {{ \\\\tune {a} d''8-> ( \\\\tune {b} d''4~ }} \\\\tune {b} d''4 ) r2. |\"\n",
    "template = \"r4 \\\\tuplet 3/2 {{ \\\\tune {a} f''8-> ( \\\\tune {b} f''4~ }} \\\\tune {b} f''4 ) r2 |\"\n",
    "template = \"r4. \\\\tuplet 3/2 {{ \\\\tune {a} g''8-> ( \\\\tune {b} g''4~ }} \\\\tune {b} g''4 ) r4. |\"\n",
    "template = \"r2. \\\\tuplet 3/2 {{ \\\\tune {a} e''8-> ( \\\\tune {b} e''4~ }} \\\\tune {b} e''4 ) |\"\n",
    "for i in range(8):\n",
    "    pair = random.sample(ratios, 2)\n",
    "    print(template.format(a=pair[0], b=pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f03b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_pitches = [\"\\\\tune 2 e'\", \"\\\\tune 3 b'\", \"\\\\tune 5 g''\", \"\\\\tune 7 d'''\",\n",
    "                 \"\\\\tune 11 a''\", \"\\\\tune 13 c'''\", \"\\\\tune 17 f''\", \"\\\\tune 19 g''\"]\n",
    "selections = [1, 2, 5, 2, 7, 1, 2, 4, 2, 3, 3, 6, 4, 4, 0, 1, 5, 6, 1, 3, 6, 7, 6, 0]\n",
    "selections = [2, 2, 3, 3, 3, 3, 0, 2, 0, 1, 2, 1, 1, 0, 1, 0]\n",
    "selections = [2, 1, 3, 2, 1, 3, 2, 3, 3, 0, 1, 1, 0, 2, 0, 0]\n",
    "template = \"\\\\tuplet 3/2 {{ {}8-> ( [ {} ) r ] }} r1 |\"\n",
    "template = \"{}8 {}8\"\n",
    "for i, j in zip(selections[:-1:2], selections[1::2]):\n",
    "    print(template.format(intro_pitches[i+4], intro_pitches[j+4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tones = np.arange(1, len(intro_pitches)+1)\n",
    "print(tones[:4])\n",
    "for i in range(4):\n",
    "    print(np.random.permutation(tones[:4]))\n",
    "\"\"\"\n",
    "[1 2 3 4 5 6 7 8]\n",
    "[8 1 5 6 7 2 4 3]\n",
    "[6 3 1 5 7 8 2 4]\n",
    "[7 2 3 5 6 1 4 8]\n",
    "[8 7 1 2 3 4 5 6]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c167bc3",
   "metadata": {},
   "source": [
    "## Another mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46045dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expo = performer.composition.score.Envelope(0.1, 3.5, 0., 0.8)\n",
    "t = np.linspace(-1, 4, 1250)\n",
    "\n",
    "data = {\n",
    "    'time': t,\n",
    "    'amplitude': expo(t)\n",
    "}\n",
    "fig = px.line(data, x='time', y='amplitude', title='Amplitude curve')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f889199",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = performer.composition.score.NoteList()\n",
    "start, duration, gap = 1.0, 0.2, 0.2\n",
    "for i in range(8):\n",
    "    notes.append(performer.composition.score.Note(start + gap * i, duration, 0.6, 110. * (i+3)))\n",
    "\n",
    "t = np.linspace(0, 8, 250*8)\n",
    "env = notes.curve(t)\n",
    "f0 = notes.freq(t)\n",
    "\n",
    "adsr = env * 90 - 100\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y = model(torch.from_numpy(f0[None, None, :].astype('float32')).cuda(), torch.from_numpy(adsr[None, None, :].astype('float32')).cuda())\n",
    "\n",
    "data = {\n",
    "    'time': t,\n",
    "    'amplitude': env\n",
    "}\n",
    "fig = px.line(data, x='time', y='amplitude', title='Amplitude curve')\n",
    "fig.show()\n",
    "\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f107e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_file = './CanisMajoris2022-Flute I.notes'\n",
    "notes = performer.composition.score.parser(events_file)\n",
    "\n",
    "start = -3.\n",
    "end = notes.notes[-1].t0 + notes.notes[-1].duration + 6.\n",
    "\n",
    "t = np.linspace(start, end, int(250*(end - start)))\n",
    "env = notes.curve(t)\n",
    "f0 = notes.freq(t)\n",
    "\n",
    "adsr = env * 90 - 100\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y = model(torch.from_numpy(f0[None, None, :].astype('float32')).cuda(), torch.from_numpy(adsr[None, None, :].astype('float32')).cuda())\n",
    "\n",
    "data = {\n",
    "    'time': t,\n",
    "    'amplitude': env\n",
    "}\n",
    "fig = px.line(data, x='time', y='amplitude', title='Amplitude curve')\n",
    "fig.show()\n",
    "\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54434502",
   "metadata": {},
   "source": [
    "## Examine real loudness and f0 envelopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = \"/home/kureta/Music/Flute Samples/02. Fantasia No. 1 in A Major, TWV 40_2.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from performer.utils.features import Loudness, get_f0\n",
    "from performer.utils.constants import N_FFT\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e539c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = Loudness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, _ = librosa.load(sample_path, sr=48000, mono=False, dtype='float32', duration=15.)\n",
    "audio = torch.from_numpy(audio)\n",
    "audio.unsqueeze_(0)\n",
    "audio = F.pad(audio, (N_FFT //2, N_FFT //2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb80169",
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness = ld.get_amp(audio)\n",
    "# f0 = get_f0(audio, fmin=31.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b77cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = loudness.squeeze().cpu().numpy()\n",
    "freq = f0.squeeze().cpu().numpy()\n",
    "t = np.arange(len(amp)) / 250\n",
    "\n",
    "source = {\n",
    "    'time': t, \n",
    "    'f0': freq,\n",
    "    'amp': amp,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9073d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=t, y=freq, name=\"F0 (Hz)\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=t, y=amp, name=\"Loudness (dB)\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Flute control parameters\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"time (seconds)\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"F0 (Hz)\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Loudness (dB)\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e37bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio.squeeze().cpu()[0], rate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d275cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y = model(f0, loudness)\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39715fe6",
   "metadata": {},
   "source": [
    "# Lilypond event stream parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883a2b9",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "### Loudness envelope\n",
    "\n",
    "- Set default values for `attack_time`, `decay_time`, `release_time`.\n",
    "  - If duration is less than the sum of those:\n",
    "    - If duration is also less than `attack_time` + `decay_time` set both to half the duration\n",
    "  - Else, keep `attack_time`, extend `decay_time` to the end if necessary\n",
    "  - Extend duration with a sustain as necessary, if all of the above fits into the duration\n",
    "- Set `peak_amp` to 1.0 and set a default `sustain_amp`\n",
    "  - Multiply the resulting envelope with constant values if there are any dynamic indicators (ex. ___pp___, ___f___, ...)\n",
    "  - if there are no dynamic indicators at the start of the piece, assume ___mf___.\n",
    "  - If there are hairpins, multiply with a line from starting dynamic to ending dynamic\n",
    "  - ___sfz___ modifies `peak_amp` / `sustain_amp` ratio.\n",
    "  - Decide what to do with other dynamic indicators when they come up.\n",
    "  - Finally, map the envelop values from 0-1 to `min_db`-`max_db`.\n",
    "- If there is a slur or tie, total duration of the envelope will be equal to the duration of the slur.\n",
    "  - Maybe add tiny attack/decays if note changes inside the slur.\n",
    "\n",
    "__Note__: not enough information in events to support phrasing slurs (slurs within slurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_f1 = './CanisMajoris2022-Flute I.notes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57577ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_note_sec(tempo):\n",
    "    return 60 * 16 / tempo\n",
    "\n",
    "def moment_to_sec(moment, tempo):\n",
    "    return whole_note_sec(tempo) * moment\n",
    "\n",
    "def midi_to_hz(midi: float) -> float:\n",
    "    return 440. * 2**((midi - 69) / 12)\n",
    "\n",
    "def hz_to_midi(hz: float) -> float:\n",
    "    return 12 * torch.log2(hz / 440) + 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio_to_interval(ratio):\n",
    "    return 12 * torch.log2(ratio)\n",
    "\n",
    "def map_from_unit(value, low, high):\n",
    "    scale = high - low\n",
    "    return value * scale + low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adsr(ta, td, tr, zero, peak, sustain, dur):\n",
    "    ts = dur - ta - td - tr\n",
    "    \n",
    "    env_a = torch.linspace(zero, peak, round(ta * 250))\n",
    "    env_d = torch.linspace(peak, sustain, round(td * 250))\n",
    "    env_sus = torch.ones(round(ts * 250)) * sustain\n",
    "    env_rel = torch.linspace(sustain, zero, round(tr * 250))\n",
    "\n",
    "    env = torch.cat([env_a, env_d, env_sus, env_rel]).cuda()\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ea95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(val: float | str):\n",
    "    if isinstance(val, str):\n",
    "        return float(val)\n",
    "    else:\n",
    "        return val\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, row):\n",
    "        self.moment = to_float(row[0])\n",
    "        self.tempo = None\n",
    "    \n",
    "    @property\n",
    "    def time(self) -> float:\n",
    "        return moment_to_sec(self.moment, self.tempo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'<{self.__class__.__name__.upper()}>\\ttime: {self.time:.2f} tempo: {self.tempo:.2f}'\n",
    "\n",
    "class Tempo(Event):\n",
    "    def __init__(self, row):\n",
    "        super().__init__(row)\n",
    "        self.tempo = to_float(row[2])\n",
    "\n",
    "class NoteOrRest(Event):\n",
    "    def __init__(self, row, tempo):\n",
    "        super().__init__(row)        \n",
    "        self.tempo = tempo\n",
    "    \n",
    "    @property\n",
    "    def dur(self):\n",
    "        return moment_to_sec(self.dur_moment, self.tempo)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        parent_repr = super().__repr__()\n",
    "        return f'{parent_repr} duration: {self.dur:.2f}'\n",
    "\n",
    "class Note(NoteOrRest):\n",
    "    def __init__(self, row, tempo):\n",
    "        super().__init__(row, tempo)\n",
    "        self.pitch = to_float(row[2])\n",
    "        self.dur_moment = to_float(row[4])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        parent_repr = super().__repr__()\n",
    "        return f'{parent_repr} pitch: {self.pitch:.2f}'\n",
    "\n",
    "    @property\n",
    "    def dur(self):\n",
    "        return moment_to_sec(self.dur_moment, self.tempo)\n",
    "\n",
    "class Rest(NoteOrRest):\n",
    "    def __init__(self, row, tempo):\n",
    "        super().__init__(row, tempo)\n",
    "        self.dur_moment = to_float(row[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb07d3",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "- We need 3 objects: pitch, gate, amp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(path: str):\n",
    "    with open(path) as csvfile:        \n",
    "        current_tempo = None\n",
    "        \n",
    "        for row in csv.reader(csvfile, delimiter='\\t'):\n",
    "            match row[1]:\n",
    "                case 'tempo':\n",
    "                    tempo = Tempo(row)\n",
    "                    current_tempo = tempo.tempo\n",
    "                    yield tempo\n",
    "                case 'note':\n",
    "                    yield Note(row, current_tempo)\n",
    "                case 'rest':\n",
    "                    yield Rest(row, current_tempo)\n",
    "                case default:\n",
    "                    yield f'<NA>\\ttime: {moment_to_sec(to_float(row[0]), current_tempo):.2f} kind: {row[1]} values: {\" - \".join(row[2:])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c10a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "pitch_vals = []\n",
    "amp_vals = []\n",
    "t_vals = []\n",
    "gates = []\n",
    "for event in parser(a_f1):\n",
    "    print(event)\n",
    "    if isinstance(event, Tempo):\n",
    "        # Nothing to do yet\n",
    "        pass\n",
    "    elif isinstance(event, Note):\n",
    "        pitch_vals.append(event.pitch)\n",
    "        amp_vals.append(0.7)\n",
    "        t_vals.append(event.time)\n",
    "        pitch_vals.append(event.pitch)\n",
    "        amp_vals.append(0.7)\n",
    "        t_vals.append(event.time + event.dur - 1e-10)\n",
    "    elif isinstance(event, Rest):\n",
    "        last_pitch = pitch_vals[-1]\n",
    "        pitch_vals.append(last_pitch)\n",
    "        amp_vals.append(0.0)\n",
    "        t_vals.append(event.time)\n",
    "        pitch_vals.append(last_pitch)\n",
    "        amp_vals.append(0.0)\n",
    "        t_vals.append(event.time + event.dur - 1e-10)\n",
    "\n",
    "pitch = np.array(pitch_vals, dtype='float32')\n",
    "amp = np.array(amp_vals, dtype='float32')\n",
    "t = np.array(t_vals, dtype='float32')\n",
    "\n",
    "interp_pitch = interpolate.interp1d(t, pitch)\n",
    "interp_amp = interpolate.interp1d(t, amp)\n",
    "\n",
    "t_new = np.linspace(t[0], t[-1], round(t[-1] * 250), dtype='float32')\n",
    "\n",
    "pitch_new = interp_pitch(t_new)\n",
    "amp_new = interp_amp(t_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f47b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = torch.from_numpy(midi_to_hz(pitch_new)).cuda()\n",
    "loudness = torch.from_numpy(map_from_unit(amp_new, -100, -15)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4714555",
   "metadata": {},
   "outputs": [],
   "source": [
    "loudness.min(), loudness.max(), amp_new.min(), amp_new.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y = model(freq[None, None, :], loudness[None, None, :]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=y.cpu(), rate=48000, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0300ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loudness.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f30f3",
   "metadata": {},
   "source": [
    "# Old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9062ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "us = music21.environment.UserSettings()\n",
    "us['musescoreDirectPNGPath'] = '/usr/bin/mscore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a8e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_hz(midi: float) -> float:\n",
    "    return 440. * 2**((midi - 69) / 12)\n",
    "\n",
    "def hz_to_midi(hz: float) -> float:\n",
    "    return 12 * torch.log2(hz / 440) + 69\n",
    "\n",
    "def ratio_to_interval(ratio):\n",
    "    return 12 * torch.log2(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e975f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adsr(ta, td, tr, zero, peak, sustain, dur):\n",
    "    ts = dur - ta - td - tr\n",
    "    \n",
    "    env_a = torch.linspace(zero, peak, round(ta * 250))\n",
    "    env_d = torch.linspace(peak, sustain, round(td * 250))\n",
    "    env_sus = torch.ones(round(ts * 250)) * sustain\n",
    "    env_rel = torch.linspace(sustain, zero, round(tr * 250))\n",
    "\n",
    "    env = torch.cat([env_a, env_d, env_sus, env_rel]).cuda()\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741beed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(ts: float, f: float):\n",
    "    t = torch.arange(int(ts * 250), dtype=torch.float32, device='cuda') / 250\n",
    "    result = torch.sin(2 * np.pi * f * t)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def sin_like(ts: torch.Tensor, f: float):\n",
    "    t = torch.arange(ts.shape[-1], dtype=torch.float32, device='cuda') / 250\n",
    "    result = torch.sin(2 * np.pi * f * t)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(music):\n",
    "    display(Image(str(music.write(\"lily.png\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_microtone(note):\n",
    "    cents = note.pitch.microtone.cents\n",
    "    prefix = ''\n",
    "    if cents > 0:\n",
    "        prefix = '+'\n",
    "    if abs(cents) >= 10:\n",
    "        note.addLyric(f'{prefix}{int(np.round(cents))}', applyRaw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.Random(123)\n",
    "beat = 0.75  # 1 beat is 0.75 seconds\n",
    "fps = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_measure(p1, p2):\n",
    "    one = music21.note.Note(quarterLength=1/3)\n",
    "    one.articulations.append(music21.articulations.Accent())\n",
    "    one.pitch.frequency = p1\n",
    "    add_microtone(one)\n",
    "\n",
    "    two = music21.note.Note(quarterLength=1/3)\n",
    "    two.pitch.frequency = p2\n",
    "    two.articulations.append(music21.articulations.Staccato())\n",
    "    add_microtone(two)\n",
    "\n",
    "    sl1 = music21.spanner.Slur([one, two])\n",
    "\n",
    "    rest1 = music21.note.Rest(1/3)\n",
    "    rest = music21.note.Rest(4)\n",
    "\n",
    "    m01 = music21.stream.Measure(number=1)\n",
    "\n",
    "    # m01.append(music21.dynamics.Dynamic('sfz'))\n",
    "    m01.append(one)\n",
    "    m01.append(two)\n",
    "    m01.append(sl1)\n",
    "    m01.append(rest1)\n",
    "    m01.append(rest)\n",
    "    \n",
    "    return m01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58769d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant = [2, 3, 5, 7, 11/2, 13/2, 17/4, 19/4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = music21.stream.Measure()\n",
    "for val in constant:\n",
    "    nn = music21.note.Note()\n",
    "    freq = midi_to_hz(ratio_to_interval(torch.tensor(val)) + 52)\n",
    "    nn.pitch.frequency = freq\n",
    "    mm.append(nn)\n",
    "mm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    [(0, 1), (2, 7), (2, 4), (5, 2), (7, 5), (7, 6), (7, 6), (2, 5)],\n",
    "    [(7, 5), (4, 3), (7, 3), (3, 0), (1, 0), (3, 1), (4, 5), (3, 0)],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d86490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = [Fraction(c) for c in constant]\n",
    "[(cons[i], cons[j]) for i, j in lines[1]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.functional.filtering import lowpass_biquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff169ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport performer.canis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "adsr = performer.canis.ADSR(0.2, 4.0)\n",
    "adsr.set_staccato()\n",
    "# adsr.set_sforzando()\n",
    "env = adsr.get_envelope_func()\n",
    "t = np.linspace(0, 8.0, 250*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, env(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    amp = env(t)\n",
    "    amp = amp * 90 - 100 - 10.\n",
    "    amp = torch.from_numpy(amp.astype('float32'))  #.cuda()\n",
    "\n",
    "    f0 = torch.ones_like(amp, device='cpu') * 440.\n",
    "\n",
    "    y = model(f0[None, None, :], amp[None, None, :])\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46689f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "many = performer.canis.ADSRList()\n",
    "delta = 0.25\n",
    "duration = 1.5\n",
    "start = 0.5\n",
    "for idx in range(5):\n",
    "    many.notes.append(performer.canis.ADSR(start, duration))\n",
    "    start += duration\n",
    "env = many.get_envelope_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    amp = env(t)\n",
    "    dynamics = performer.canis.get_line(many.notes[0].start, duration * 5, 1, 0.5)\n",
    "    def dynamo(t):\n",
    "        return np.minimum(1, np.maximum(dynamics(t), 0))\n",
    "    amp = amp * dynamo(t)\n",
    "    amp = amp * 90 - 100 - 10.\n",
    "    amp = torch.from_numpy(amp.astype('float32'))  # .cuda()\n",
    "\n",
    "    f0 = torch.ones_like(amp, device='cpu') * 440.\n",
    "\n",
    "    y = model(f0[None, None, :], amp[None, None, :])\n",
    "Audio(y.cpu().squeeze(), rate=48000, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d52f2",
   "metadata": {},
   "source": [
    "## WHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = adsr(0.1, 0.7, 0.01, -100, -8, -48, 1.5 * beat).cpu()\n",
    "silence = torch.ones(round(3.5 * beat * fps), device='cpu') * -100.\n",
    "env = torch.cat([amp, silence], dim=-1)\n",
    "\n",
    "\n",
    "s = music21.stream.Score(id='mainScore')\n",
    "part0 = music21.stream.Part(id='part0')\n",
    "part1 = music21.stream.Part(id='part1')\n",
    "\n",
    "\n",
    "ys = []\n",
    "parts = [part0, part1]\n",
    "lines = [\n",
    "#     [(0, 1), (2, 4), (7, 4), (5, 2), (7, 5), (7, 6), (7, 6), (7, 6)],\n",
    "#     [(7, 5), (2, 3), (7, 3), (3, 0), (1, 0), (3, 1), (4, 5), (3, 0)],\n",
    "    [(0, 1), (2, 7), (2, 4), (5, 2), (7, 5), (7, 6), (7, 6), (2, 5)],\n",
    "    [(7, 5), (4, 3), (7, 3), (3, 0), (1, 0), (3, 1), (4, 5), (3, 0)],\n",
    "]\n",
    "for part, line in zip(parts, lines):\n",
    "    oll = []\n",
    "    for idx1, idx2 in line:\n",
    "        with torch.inference_mode():\n",
    "            p1 = midi_to_hz(ratio_to_interval(torch.tensor(constant[idx1])) + 52)\n",
    "            p2 = midi_to_hz(ratio_to_interval(torch.tensor(constant[idx2])) + 52)\n",
    "            mezura = build_measure(p1, p2)\n",
    "            # if j % 3 == 2:\n",
    "            #     mezura.append(music21.layout.SystemLayout(isNew=True))\n",
    "            part.append(mezura)\n",
    "            \n",
    "            f0 = torch.ones_like(env) * p2\n",
    "            f0[:int(beat*0.333*fps)] = p1\n",
    "            y = model(f0[None, None, :], env[None, None, :])\n",
    "            oll.append(y)\n",
    "\n",
    "    ys.append(torch.cat(oll, dim=-1).cpu().numpy().squeeze())\n",
    "\n",
    "\n",
    "tempo = music21.tempo.MetronomeMark(referent=1.0, number=90.0)\n",
    "\n",
    "for part in parts:\n",
    "    part.measure(1).insert(tempo)\n",
    "    part.insert(0, music21.meter.TimeSignature('5/4'))\n",
    "    s.insert(0, part)\n",
    "\n",
    "f0 = midi_to_hz(torch.ones_like(env, device='cpu') * 51-12)\n",
    "amp = adsr(0.1, 0.7, 0.01, -100, -3, -48, 2.5 * beat).cpu()\n",
    "silence = torch.ones(round(2.5 * beat * fps), device='cpu') * -100.\n",
    "env = torch.cat([amp, silence], dim=-1)\n",
    "oll = []\n",
    "for _ in range(8):\n",
    "    with torch.inference_mode():\n",
    "        y = model(f0[None, None, :], env[None, None, :])  # * (torch.randn(1, device='cuda') * 0.25 + 1))\n",
    "        oll.append(y)\n",
    "\n",
    "ys.append(torch.cat(oll, dim=-1).cpu().numpy().squeeze())\n",
    "\n",
    "s.show()\n",
    "Audio(data=sum(ys), rate=48000, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d051cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.min(), env.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957b229",
   "metadata": {},
   "source": [
    "## Braids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a4601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tones = np.arange(1, 4)\n",
    "print(tones)\n",
    "for i in range(4):\n",
    "    print(np.random.permutation(tones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d63c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8a0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bjorklund(steps, pulses):\n",
    "    steps = int(steps)\n",
    "    pulses = int(pulses)\n",
    "    if pulses > steps:\n",
    "        raise ValueError    \n",
    "    pattern = []    \n",
    "    counts = []\n",
    "    remainders = []\n",
    "    divisor = steps - pulses\n",
    "    remainders.append(pulses)\n",
    "    level = 0\n",
    "    while True:\n",
    "        counts.append(divisor // remainders[level])\n",
    "        remainders.append(divisor % remainders[level])\n",
    "        divisor = remainders[level]\n",
    "        level = level + 1\n",
    "        if remainders[level] <= 1:\n",
    "            break\n",
    "    counts.append(divisor)\n",
    "    \n",
    "    def build(level):\n",
    "        if level == -1:\n",
    "            pattern.append(0)\n",
    "        elif level == -2:\n",
    "            pattern.append(1)         \n",
    "        else:\n",
    "            for i in range(0, counts[level]):\n",
    "                build(level - 1)\n",
    "            if remainders[level] != 0:\n",
    "                build(level - 2)\n",
    "    \n",
    "    build(level)\n",
    "    i = pattern.index(1)\n",
    "    pattern = pattern[i:] + pattern[0:i]\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43519501",
   "metadata": {},
   "outputs": [],
   "source": [
    "bjorklund(10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb73b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bjorklund(8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ffee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
